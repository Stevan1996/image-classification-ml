{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f8cd17b-2f47-4d2a-86b8-e7eaa6473f5c",
   "metadata": {},
   "source": [
    "### Wie implementiert man eine KI?\n",
    "Es gibt verschiedene Ansätze eine KI zu realisieren.\n",
    "* statischer Ansatz: Support Vector Machines (SVM), Clustering-Verfahren\n",
    "* evolutionärer Ansatz: genetische Algorithmen\n",
    "* konnektionistischer Ansatz: künstliche neuronale Netze (KNN) <- aktuell State of the Art\n",
    "\n",
    "### Was ist ein künstliches neuronales Netz?\n",
    "Ein KNN lässt sich durch einen Graphen darstellen.\n",
    "![Beispiel neuronales Netz](img/KNN.png)\n",
    "* Input des Netzes nennen sich __Features__ (ganz selten Kovariate)\n",
    "* Kanten haben ein Gewicht $w$ => trainierbare Parameter\n",
    "* Knoten sind die Neuronen\n",
    "* Neuronen enthalten eine Aktivierungsfunktion $f$\n",
    "* KNNs approximieren im Grunde eine Funktion, die durch die Daten beschrieben wird\n",
    "\n",
    "Seien $x \\in \\mathbb{R}$ ein Feature, $f_{A/B}: \\mathbb{R} \\to \\mathbb{R}$ die Aktivierungsfunktionen der Neuronen und $w \\in \\mathbb{R}$ das Gewicht. Dann lässt dich das obige neuronale $k$ ausdrücken durch:\n",
    "$k(x) = f_B(w f_A(x))$\n",
    "\n",
    "![künstliches neuronales Netz](img/kuenstliche-neuronale-Netze.jpg)\n",
    "Neuronale Netze sind in Schichten aufgeteilt:\n",
    "* __Eingabeschicht (Input Layer):__ Größe des Input Layer wird durch die Daten bestimmt\n",
    "* __Verbogene Schicht (Hidden Layer):__ Größe und Anzahl der Schichten sind variabel. Ab einer bestimmten Anzahl von Hidden Layers spricht man von Deep Learning.\n",
    "* __Ausgabeschicht (Output Layer):__ Größe wird durch das Problem definiert.\n",
    "\n",
    "### Wie lernt ein KNN?\n",
    "Ein KNN lernt, indem es während des Trainings seine Netzgewichte $w_i$ anpasst. Während des Trainings erhält das Netz einen Datensatz mit den gewünschten Ergebnis (Labels). Dabei wird ein Datenpunkt aus dem Datensatz in das Netz reingegeben. Die Ausgabe des Netzes wird mit dann mit dem Label verglichen. Bei einer Abweichung werden die Netzgewichte so verändert, dass bei der nächsten Auswertung des Datenpunkts die Netzausgabe näher zum Label ist.\n",
    "Ein kompletter Durchlauf des Trainingsdatensatzes nennt man __Epoche__.\n",
    "\n",
    "#### Underfitting\n",
    "Falls das neuronale Netz nicht genügend trainierbare Parameter hat, um das Problem aus dem Datensatz zu lernen, nennt man das Phänomen Underfitting. Das neuronale Netz muss in dem Fall vergrößert werden.\n",
    "\n",
    "#### Overfitting\n",
    "Das KNN performt während des Tranings sehr gut. In der Produktion ist die Fehlerrate ungewöhnlich hoch => Das KNN hat genug Kapazität, um die Beispiele aus dem Datensatz auswendig zu lernen.\n",
    "Um das KNN zu zwingen, sich zu generalisieren, kann man das KNN verkleinern, den Datensatz vergrößern und/oder Regularisieren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbe519f-db6a-4f19-9e68-0a6f170ffb4e",
   "metadata": {},
   "source": [
    "### Exkurs: Wie werden Bilder digital dargestellt?\n",
    "#### Fall 1: Schwarz-Weiß-Bild\n",
    "Ein Schwarz-Weiß-Bild lässt sich durch eine Matrix darstellen. Der Matrixeintrag $m_{xy} \\in [0; 255]$ gibt an, wie hell/dunkel der Pixel an der Stelle $x, y$ ist.\n",
    "\n",
    "#### Fall 2: Farbbilder\n",
    "Farbbilder haben drei Farbkanäle (RGB). Jeder Farbkanal ist eine Matrix. Durch die Überlagerung der drei Farbkanäle erhält man das Farbbild.\n",
    "![Tensor](img/Epsilontensor.svg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405e855d-5e40-4a1b-9f2d-8832ac916048",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = cv2.imread(\"img/color_image.jpg\", 1)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf4cb9f-141d-4970-993e-a99467d51848",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img[:, :, 0], cmap=\"Reds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191596e1-d37d-402b-a3b2-58c4eb04c816",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img[:, :, 1], cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf93da41-4967-4ee7-83b7-619bf3529a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img[:, :, 2], cmap=\"Greens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77a57ac-2d0c-4cab-9b3d-52058fa267e8",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks (CNN)\n",
    "Bisher können in den neuronalen Netzen nur Vektoren eingegeben werden. Bilder haben jedoch die Form einer Matrix oder eines Tensors. Man könnte die Bilder vektorisieren (flatten). Der räumliche Informationsgehalt eines Pixel wird jedoch dabei verringert.\n",
    "Stattdessen haben sich Convolutional Neural Networks (CNN) als State-of-the-Art für Bilddaten etabliert.\n",
    "![CNN](img/CNN.jpg)\n",
    "\n",
    "\n",
    "\n",
    "#### Convolution-Operation\n",
    "Bei der Convolution-Operation haben wir einen __Kernel__. Die Einträge im Kernel sind die trainierbaren Parameter des CNNs.\n",
    "![convolution operation](img/convolution_operation.png)\n",
    "\n",
    "Die Convolution-Operation verkleinert die Dimension des Bilds.\n",
    "\n",
    "\n",
    "#### Pooling-Operation\n",
    "Bei der Pooling-Operation werden die Informationen aus einem Teilbereich des Bilds weiter komprimiert.\n",
    "![Max pooling operation](img/pooling.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e38b43f-b8e1-486c-b9be-f508f9e9269c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "import keras\n",
    "from keras import layers\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86afa881-200f-4077-9d37-2d1b31b5a027",
   "metadata": {},
   "outputs": [],
   "source": [
    "(training_set, training_labels), (test_set, test_labels) = keras.datasets.mnist.load_data()\n",
    "training_set = training_set.astype(\"float32\") / 255.0\n",
    "test_set = test_set.astype(\"float32\") / 255.0\n",
    "\n",
    "training_set = np.expand_dims(training_set, -1)\n",
    "test_set = np.expand_dims(test_set, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837d4591-3804-4a13-9789-f458cf4e23d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20f5369-d634-4145-b1a4-b445f09c0c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(training_set[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e37765-9a4b-4b53-a943-175bf9001bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9385c7c5-8954-47b4-a984-494776f8cbf7",
   "metadata": {},
   "source": [
    "#### Dokumentation zu den Keras Layers\n",
    "__Convolution:__ \\\n",
    "https://keras.io/api/layers/convolution_layers/convolution2d/ \\\n",
    "https://keras.io/api/layers/convolution_layers/convolution3d/\n",
    "\n",
    "__Pooling:__ \\\n",
    "https://keras.io/api/layers/pooling_layers/max_pooling2d/ \\\n",
    "https://keras.io/api/layers/pooling_layers/max_pooling3d/ \\\n",
    "https://keras.io/api/layers/pooling_layers/average_pooling2d/ \\\n",
    "https://keras.io/api/layers/pooling_layers/average_pooling3d/\n",
    "\n",
    "__Fully Connected:__ \\\n",
    "https://keras.io/api/layers/core_layers/dense/\n",
    "\n",
    "__Regularisierung:__ \\\n",
    "https://keras.io/api/layers/regularization_layers/dropout/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f896d55-6b17-4b07-badf-d32babd3bfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = keras.Input(shape=training_set.shape[1:])\n",
    "hidden_layer = layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\")(input)\n",
    "hidden_layer = layers.MaxPooling2D(pool_size=(2, 2))(hidden_layer)\n",
    "hidden_layer = layers.Flatten()(hidden_layer)\n",
    "hidden_layer = layers.Dropout(0.5)(hidden_layer)\n",
    "output = layers.Dense(10, activation=\"softmax\")(hidden_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e55b01-1c91-4fbf-93fb-aa74506f71ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_classificator = keras.Model(inputs=input, outputs=output, name=\"conv_net_classification\")\n",
    "img_classificator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6ca75f-5c4b-44a6-a8bb-f7c789e5a981",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_classificator.compile(loss=keras.losses.CategoricalCrossentropy(), optimizer=\"adam\", metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e74298-8f72-4e8a-80fa-2b5d49a1043f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labels = keras.utils.to_categorical(training_labels, 10)\n",
    "test_labels = keras.utils.to_categorical(test_labels, 10)\n",
    "training_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae25e2f-cf8f-4b7c-b386-b9b1bfdb77cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = img_classificator.fit(training_set, training_labels, batch_size=128, epochs=5, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209ef642-f306-4415-9378-5d517337063a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"acc\"])\n",
    "plt.plot(history.history[\"val_acc\"])\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend([\"train\", \"val\"], loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c759246-0197-45ac-8f94-becb632703d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = img_classificator.evaluate(test_set, test_labels)\n",
    "score[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42b6670-7c43-49e1-8d2a-f08464b83936",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = img_classificator.predict(test_set[1:2])\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b929e4-161d-469e-89e8-49dbb1147e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e43fa3-4781-4252-919b-81130cd7e5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_set[1], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b88f8b0-5a6e-4224-99e4-df53e8a3d6f4",
   "metadata": {},
   "source": [
    "### Weitere Datensätze\n",
    "__Fashion MNIST__: https://keras.io/api/datasets/fashion_mnist/ \\\n",
    "__CIFAR10__: https://keras.io/api/datasets/cifar10/ \\\n",
    "__CIFAR100__: https://keras.io/api/datasets/cifar100/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
